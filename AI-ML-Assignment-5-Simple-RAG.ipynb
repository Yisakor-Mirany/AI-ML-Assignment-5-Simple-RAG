{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b596b1d-db22-4b42-8698-ed3b38d2c69d",
   "metadata": {},
   "source": [
    "## 1. Knowledge Base Creation\n",
    "\n",
    "This section defines the custom Knowledge Base (KB) used for the RAG system.  \n",
    "The topic is a fictional late-2024 research paper titled:\n",
    "\n",
    "**“Adaptive Retrieval-Augmented Generation for Memory-Constrained Edge Devices” (Lin & Moretti, 2024)**\n",
    "\n",
    "The KB contains three detailed paragraphs summarizing the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef07f97-7a17-466c-9f99-3ec4843b38b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn late 2024, Lin and Moretti introduced an approach called Adaptive RAG, designed specifically for deployment on memory-constrained edge devices such as IoT sensors, mobile robots, and compact industrial controllers. Traditional RAG systems rely on large vector stores and GPU-accelerated embeddings, making them difficult to run on hardware with limited RAM and no dedicated accelerators. Adaptive RAG addresses this by using low-dimensional (128–256d) embeddings and a dynamic sliding-window vector store that automatically discards less relevant vectors to maintain a constant memory footprint. The system also employs context-sensitive retrieval, selecting only the top-1 or top-2 chunks to minimize token expansion during generation.\\n\\nThe paper introduces a novel module called the Relevance-Weighted Cache (RWC), which tracks which knowledge fragments have been most frequently and recently used by the model. When a new query arrives, the RWC assigns a relevance score based on recency, similarity, and query intent. If the cache predicts that a chunk is highly likely to be needed again, the system stores it in a dedicated micro-index separate from the sliding-window store. This improves retrieval latency by up to 40% compared to baseline compact RAG systems. The authors evaluate the method on an edge inference benchmark using a Raspberry Pi 4 and several microcontrollers, showing that Adaptive RAG maintains 92% of the accuracy of full RAG systems while reducing memory usage by 65%.\\n\\nThe study concludes that the proposed architecture significantly improves on-device reasoning without requiring cloud offloading. However, the authors note that its performance depends heavily on the quality of the lightweight embedding model and the tuning of the sliding-window size. Future work includes knowledge distillation for ultra-small embedding models, quantized vector stores, and integrating temporal reasoning for edge devices that interact with time-dependent sensor data. Lin and Moretti argue that Adaptive RAG may become a foundational technique for autonomous systems operating in remote or bandwidth-limited environments.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_text = \"\"\"\n",
    "In late 2024, Lin and Moretti introduced an approach called Adaptive RAG, designed specifically for deployment on memory-constrained edge devices such as IoT sensors, mobile robots, and compact industrial controllers. Traditional RAG systems rely on large vector stores and GPU-accelerated embeddings, making them difficult to run on hardware with limited RAM and no dedicated accelerators. Adaptive RAG addresses this by using low-dimensional (128–256d) embeddings and a dynamic sliding-window vector store that automatically discards less relevant vectors to maintain a constant memory footprint. The system also employs context-sensitive retrieval, selecting only the top-1 or top-2 chunks to minimize token expansion during generation.\n",
    "\n",
    "The paper introduces a novel module called the Relevance-Weighted Cache (RWC), which tracks which knowledge fragments have been most frequently and recently used by the model. When a new query arrives, the RWC assigns a relevance score based on recency, similarity, and query intent. If the cache predicts that a chunk is highly likely to be needed again, the system stores it in a dedicated micro-index separate from the sliding-window store. This improves retrieval latency by up to 40% compared to baseline compact RAG systems. The authors evaluate the method on an edge inference benchmark using a Raspberry Pi 4 and several microcontrollers, showing that Adaptive RAG maintains 92% of the accuracy of full RAG systems while reducing memory usage by 65%.\n",
    "\n",
    "The study concludes that the proposed architecture significantly improves on-device reasoning without requiring cloud offloading. However, the authors note that its performance depends heavily on the quality of the lightweight embedding model and the tuning of the sliding-window size. Future work includes knowledge distillation for ultra-small embedding models, quantized vector stores, and integrating temporal reasoning for edge devices that interact with time-dependent sensor data. Lin and Moretti argue that Adaptive RAG may become a foundational technique for autonomous systems operating in remote or bandwidth-limited environments.\n",
    "\"\"\"\n",
    "kb_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5c2c8-c837-430d-bc94-7c1856b3641b",
   "metadata": {},
   "source": [
    "## 2. Embedding & Indexing\n",
    "\n",
    "We chunk the KB into smaller segments, compute embeddings using a\n",
    "Sentence Transformer model, and store them in a simple vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ee8e6a-86e6-4610-9b73-3d2ead5f26e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (5.1 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.9.1-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.13/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.13/site-packages (from sentence-transformers) (1.15.3)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.13/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (3.17.0)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading faiss_cpu-1.12.0-cp313-cp313-macosx_14_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Downloading torch-2.9.1-cp313-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: safetensors, hf-xet, faiss-cpu, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [sentence-transformers]/8\u001b[0m [sentence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed faiss-cpu-1.12.0 hf-xet-1.2.0 huggingface-hub-0.36.0 safetensors-0.6.2 sentence-transformers-5.1.2 tokenizers-0.22.1 torch-2.9.1 transformers-4.57.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae3cd3c10e246b18534bbb708183ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde185d83327459aa54440cee5ab8692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459fbb989db64b8ba6e1c90c962d9341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be56ad8f423479f973cd72c40c56212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a08af86374430cb92468186faaae83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a8ede7cb814ac69f82522b0e587cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e81b7dd2da42178342a3ff13bcab90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2be6ddef2c4881971650a3d72f5567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c1b9afb75a460a80d9a8857c5a2704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096f140e2cf44da78eeffcde44224347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c00bde911d4afa99c9c57523269b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['In late 2024, Lin and Moretti introduced an approach called Adaptive RAG, designed specifically for deployment on memory-constrained edge devices such as IoT sensors, mobile robots, and compact industrial controllers. Traditional RAG systems rely on large vector stores and GPU-accelerated embeddings, making them difficult to run on hardware with limited RAM and no dedicated accelerators. Adaptive RAG addresses this by using low-dimensional (128–256d) embeddings and a dynamic sliding-window vector store that automatically discards less relevant vectors to maintain a constant memory footprint. The system also employs context-sensitive retrieval, selecting only the top-1 or top-2 chunks to minimize token expansion during generation.',\n",
       " 'The paper introduces a novel module called the Relevance-Weighted Cache (RWC), which tracks which knowledge fragments have been most frequently and recently used by the model. When a new query arrives, the RWC assigns a relevance score based on recency, similarity, and query intent. If the cache predicts that a chunk is highly likely to be needed again, the system stores it in a dedicated micro-index separate from the sliding-window store. This improves retrieval latency by up to 40% compared to baseline compact RAG systems. The authors evaluate the method on an edge inference benchmark using a Raspberry Pi 4 and several microcontrollers, showing that Adaptive RAG maintains 92% of the accuracy of full RAG systems while reducing memory usage by 65%.',\n",
       " 'The study concludes that the proposed architecture significantly improves on-device reasoning without requiring cloud offloading. However, the authors note that its performance depends heavily on the quality of the lightweight embedding model and the tuning of the sliding-window size. Future work includes knowledge distillation for ultra-small embedding models, quantized vector stores, and integrating temporal reasoning for edge devices that interact with time-dependent sensor data. Lin and Moretti argue that Adaptive RAG may become a foundational technique for autonomous systems operating in remote or bandwidth-limited environments.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install sentence-transformers faiss-cpu transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Simple chunking\n",
    "chunks = kb_text.split(\"\\n\\n\")\n",
    "chunks = [c.strip() for c in chunks if c.strip()]\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f08c7cbd-79d7-4bde-80ea-f213ed711dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks stored: 3\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "embeddings = model.encode(chunks)\n",
    "\n",
    "# Convert to float32 for FAISS\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"Chunks stored:\", len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff683bc-3f1a-4061-90df-11be0f1d6b1e",
   "metadata": {},
   "source": [
    "## 3. Retrieval\n",
    "\n",
    "This function embeds the user query and performs a similarity search\n",
    "against the knowledge base using FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb135f97-028e-40f8-9543-165e22b225d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=2):\n",
    "    query_emb = model.encode([query]).astype(\"float32\")\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "    \n",
    "    retrieved_chunks = [chunks[i] for i in indices[0]]\n",
    "    return retrieved_chunks, distances[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029048eb-b701-4255-a954-a3f189b17ea2",
   "metadata": {},
   "source": [
    "## 4. Generation (Augmented LLM Prompting)\n",
    "\n",
    "We combine the user's query with retrieved KB context\n",
    "and pass it to a small LLM for final answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce37289-e3d3-46a6-9eb6-c8bc8de12d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d449069ba4214ea4b11d266b06394701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafaee75513146af914652146f27838c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360ef690e2924ef6a7996cf2cb1421b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1f9dcb160143c98b5392be83584c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff423d2aa40642e4b1ff176613269d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7150909f21463498e76292fcfd4494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0835cd9af7141e4aed55f60e24da068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load a **small T5 model** (lightweight for demos)\n",
    "llm_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "model_llm = AutoModelForSeq2SeqLM.from_pretrained(llm_name)\n",
    "\n",
    "def generate_answer(query):\n",
    "    context, _ = retrieve(query, k=2)\n",
    "    context_text = \"\\n\\n\".join(context)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model_llm.generate(**inputs, max_length=150)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True), context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da9aa30-f7c1-4da0-b665-c4aace0a95bc",
   "metadata": {},
   "source": [
    "## 5. Test Cases\n",
    "\n",
    "Three tests required by the assignment:\n",
    "1. Factual (answer appears in KB)\n",
    "2. General/Foil (not in KB)\n",
    "3. Synthesis (requires multiple chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "094dc0df-a13f-4670-825d-e1df84ad40c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER 1: to track which knowledge fragments have been most frequently and recently used by the model\n",
      "\n",
      "RETRIEVED CHUNKS: ['The paper introduces a novel module called the Relevance-Weighted Cache (RWC), which tracks which knowledge fragments have been most frequently and recently used by the model. When a new query arrives, the RWC assigns a relevance score based on recency, similarity, and query intent. If the cache predicts that a chunk is highly likely to be needed again, the system stores it in a dedicated micro-index separate from the sliding-window store. This improves retrieval latency by up to 40% compared to baseline compact RAG systems. The authors evaluate the method on an edge inference benchmark using a Raspberry Pi 4 and several microcontrollers, showing that Adaptive RAG maintains 92% of the accuracy of full RAG systems while reducing memory usage by 65%.', 'In late 2024, Lin and Moretti introduced an approach called Adaptive RAG, designed specifically for deployment on memory-constrained edge devices such as IoT sensors, mobile robots, and compact industrial controllers. Traditional RAG systems rely on large vector stores and GPU-accelerated embeddings, making them difficult to run on hardware with limited RAM and no dedicated accelerators. Adaptive RAG addresses this by using low-dimensional (128–256d) embeddings and a dynamic sliding-window vector store that automatically discards less relevant vectors to maintain a constant memory footprint. The system also employs context-sensitive retrieval, selecting only the top-1 or top-2 chunks to minimize token expansion during generation.']\n",
      "\n",
      "ANSWER 2: Adaptive RAG maintains 92% of the accuracy of full RAG systems while reducing memory usage by 65%\n",
      "\n",
      "RETRIEVED CHUNKS: ['In late 2024, Lin and Moretti introduced an approach called Adaptive RAG, designed specifically for deployment on memory-constrained edge devices such as IoT sensors, mobile robots, and compact industrial controllers. Traditional RAG systems rely on large vector stores and GPU-accelerated embeddings, making them difficult to run on hardware with limited RAM and no dedicated accelerators. Adaptive RAG addresses this by using low-dimensional (128–256d) embeddings and a dynamic sliding-window vector store that automatically discards less relevant vectors to maintain a constant memory footprint. The system also employs context-sensitive retrieval, selecting only the top-1 or top-2 chunks to minimize token expansion during generation.', 'The paper introduces a novel module called the Relevance-Weighted Cache (RWC), which tracks which knowledge fragments have been most frequently and recently used by the model. When a new query arrives, the RWC assigns a relevance score based on recency, similarity, and query intent. If the cache predicts that a chunk is highly likely to be needed again, the system stores it in a dedicated micro-index separate from the sliding-window store. This improves retrieval latency by up to 40% compared to baseline compact RAG systems. The authors evaluate the method on an edge inference benchmark using a Raspberry Pi 4 and several microcontrollers, showing that Adaptive RAG maintains 92% of the accuracy of full RAG systems while reducing memory usage by 65%.']\n",
      "\n",
      "ANSWER 3: Adaptive RAG maintains 92% of the accuracy of full RAG systems while reducing memory usage by 65%\n",
      "\n",
      "RETRIEVED CHUNKS: ['The paper introduces a novel module called the Relevance-Weighted Cache (RWC), which tracks which knowledge fragments have been most frequently and recently used by the model. When a new query arrives, the RWC assigns a relevance score based on recency, similarity, and query intent. If the cache predicts that a chunk is highly likely to be needed again, the system stores it in a dedicated micro-index separate from the sliding-window store. This improves retrieval latency by up to 40% compared to baseline compact RAG systems. The authors evaluate the method on an edge inference benchmark using a Raspberry Pi 4 and several microcontrollers, showing that Adaptive RAG maintains 92% of the accuracy of full RAG systems while reducing memory usage by 65%.', 'In late 2024, Lin and Moretti introduced an approach called Adaptive RAG, designed specifically for deployment on memory-constrained edge devices such as IoT sensors, mobile robots, and compact industrial controllers. Traditional RAG systems rely on large vector stores and GPU-accelerated embeddings, making them difficult to run on hardware with limited RAM and no dedicated accelerators. Adaptive RAG addresses this by using low-dimensional (128–256d) embeddings and a dynamic sliding-window vector store that automatically discards less relevant vectors to maintain a constant memory footprint. The system also employs context-sensitive retrieval, selecting only the top-1 or top-2 chunks to minimize token expansion during generation.']\n"
     ]
    }
   ],
   "source": [
    "### Test Case 1 — Factual\n",
    "q1 = \"What is the purpose of the Relevance-Weighted Cache?\"\n",
    "a1, c1 = generate_answer(q1)\n",
    "print(\"ANSWER 1:\", a1)\n",
    "print(\"\\nRETRIEVED CHUNKS:\", c1)\n",
    "\n",
    "### Test Case 2 — Foil / Not in KB\n",
    "q2 = \"How does Adaptive RAG compare to ChatGPT-4?\"\n",
    "a2, c2 = generate_answer(q2)\n",
    "print(\"\\nANSWER 2:\", a2)\n",
    "print(\"\\nRETRIEVED CHUNKS:\", c2)\n",
    "\n",
    "### Test Case 3 — Synthesis\n",
    "q3 = \"How does Adaptive RAG reduce memory usage while improving retrieval latency?\"\n",
    "a3, c3 = generate_answer(q3)\n",
    "print(\"\\nANSWER 3:\", a3)\n",
    "print(\"\\nRETRIEVED CHUNKS:\", c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947485f-d265-44b6-a11a-a778fede4d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
